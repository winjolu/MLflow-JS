{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b03a7a7-8221-4eb1-857b-30e8db276c5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mlflow' has no attribute 'login'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Run the login function to authenticate with Databricks CE\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'mlflow' has no attribute 'login'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Run the login function to authenticate with Databricks CE\n",
    "mlflow.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ac3e8-915b-4a20-b1ab-7d28f387cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/29 02:00:34 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run awesome-seal-612 at: https://community.cloud.databricks.com/ml/experiments/2597702965538188/runs/a4d786667a4f4e408d7a75d0062d959b.\n",
      "2024/08/29 02:00:34 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: https://community.cloud.databricks.com/ml/experiments/2597702965538188.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dummy experiment from tutorial\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "mlflow.set_experiment(\"/check-databricks-connection\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"foo\", 1)\n",
    "    mlflow.log_metric(\"bar\", 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f08a9e-e7f5-4358-b799-635a4b95faab",
   "metadata": {},
   "source": [
    "## Simple Model and MLflow Experiment with Iris Dataset\n",
    "Uses classic Iris dataset and RandomForest algo to train an ML model for classifying species of irises.\n",
    "\n",
    "### Key Steps\n",
    "1. Load dataset\n",
    "2. Split data set into training and testing subsets (0.7, 0.3)\n",
    "3. Train a Random Forest Classifier model with a range of n_estimators\n",
    "4. Make predictions on the test data for each\n",
    "5. Calculate accuracy for each model and track in MLflow \n",
    "6. Log the best model and log train_n_track.ipynb to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b36330-1b98-497b-ae82-bd814096f15a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 50, test set accuracy = 0.9778\n",
      "n_estimators = 100, test set accuracy = 0.9778\n",
      "n_estimators = 200, test set accuracy = 0.9778\n",
      "n_estimators = 500, test set accuracy = 0.9778\n",
      "\n",
      "Average accuracy across all models: 0.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/29 20:41:58 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.15.1/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n",
      "2024/08/29 20:42:00 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: n_estimators = 50, accuracy = 0.9778\n",
      "Run ID: 509923ed07c345b8894918effad86f63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/29 20:42:01 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run smiling-turtle-739 at: https://community.cloud.databricks.com/ml/experiments/3146434618114174/runs/509923ed07c345b8894918effad86f63.\n",
      "2024/08/29 20:42:01 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: https://community.cloud.databricks.com/ml/experiments/3146434618114174.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# kill old processes\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# set up mlflow tracking\n",
    "mlflow.set_tracking_uri('databricks')\n",
    "mlflow.set_experiment('/iris-random-forest')\n",
    "\n",
    "# load iris dataset\n",
    "data = load_iris()\n",
    "# split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=23)\n",
    "\n",
    "# list of n_estimators to try\n",
    "n_estimators_list = [50, 100, 200, 500]\n",
    "\n",
    "# variables to store results\n",
    "accuracies = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_n_estimators = None\n",
    "\n",
    "# start an mlflow experiment\n",
    "with mlflow.start_run() as active_run:  # active_run is now defined\n",
    "    for n in n_estimators_list:\n",
    "        # initialize and train the model\n",
    "        model = RandomForestClassifier(n_estimators=n, random_state=23)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # predict on the test set\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # calculate accuracy on the test set\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # log the accuracy to mlflow\n",
    "        mlflow.log_param(f'n_estimators_{n}', n)\n",
    "        mlflow.log_metric(f'accuracy_{n}', accuracy)\n",
    "        \n",
    "        # print the accuracy for the current model\n",
    "        print(f'n_estimators = {n}, test set accuracy = {accuracy:.4f}')\n",
    "        \n",
    "        # check if this is the best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            best_n_estimators = n\n",
    "\n",
    "    # calculate and log the average accuracy\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    mlflow.log_metric('average_accuracy', avg_accuracy)\n",
    "    print(f'\\nAverage accuracy across all models: {avg_accuracy:.4f}')\n",
    "    \n",
    "    # log and print the best model's accuracy\n",
    "    mlflow.log_param('best_n_estimators', best_n_estimators)\n",
    "    mlflow.log_metric('best_accuracy', best_accuracy)\n",
    "    mlflow.sklearn.log_model(best_model, 'best_model')\n",
    "    \n",
    "    print(f'Best model: n_estimators = {best_n_estimators}, accuracy = {best_accuracy:.4f}')\n",
    "    print('Run ID:', active_run.info.run_id)  # use active_run.info.run_id to access the run ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b38fa-5381-48da-8a5c-331e04ffe3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4516b2-4aab-40ad-ac99-5d35f622f5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
